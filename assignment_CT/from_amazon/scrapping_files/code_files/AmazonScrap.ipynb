{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MkPSqFA3dabx"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "h8EtqPWNdWAk"
      },
      "outputs": [],
      "source": [
        "URL = 'https://www.amazon.in/s?k=smart+lock&page=7&crid=3MNJ4J5A0A3FY&qid=1730092715&sprefix=smart+lock%2Caps%2C209&ref=sr_pg_7'\n",
        "API_KEY = '53fde5daee9e3fd8b128656faaf77e49'\n",
        "\n",
        "# ScraperAPI setup\n",
        "api_url = f'http://api.scraperapi.com?api_key={API_KEY}&url={URL}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "t8MIwAZYd3hz"
      },
      "outputs": [],
      "source": [
        "# HTTP Request\n",
        "webpage = requests.get(api_url)\n",
        "# Soup Object containiang all data\n",
        "soup = BeautifulSoup(webpage.content, \"html.parser\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "KuKRSVPpd7hw"
      },
      "outputs": [],
      "source": [
        "links = soup.find_all(\"a\", attrs={'class': 'a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHjVbkfyd_Bm",
        "outputId": "21b5dd65-3cd2-49c0-cbf0-fa945d3b3d37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "36\n"
          ]
        }
      ],
      "source": [
        "print(len(links))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rj__mQ7Le-j0",
        "outputId": "f2546cb0-cb2f-48ba-b56b-57c45314a87b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data saved to product_data.csv\n"
          ]
        }
      ],
      "source": [
        "# Initialize the list to store product data\n",
        "product_data = []\n",
        "\n",
        "# Function to extract Best Seller Rank\n",
        "def fetch_best_seller_rank(soup):\n",
        "    try:\n",
        "        table = soup.find(\"table\", {\"id\": \"productDetails_detailBullets_sections1\"})\n",
        "        rows = table.find_all(\"tr\") if table else []\n",
        "\n",
        "        for row in rows:\n",
        "            header = row.find(\"th\", class_=\"a-color-secondary a-size-base prodDetSectionEntry\")\n",
        "            value = row.find(\"span\")\n",
        "\n",
        "            if header and \"Best Sellers Rank\" in header.text:\n",
        "                ranks = [rank.strip() for rank in value.text.split(\"\\n\") if rank.strip()]\n",
        "                return ranks[0] if ranks else \"N/A\"  # Select the primary rank\n",
        "    except AttributeError:\n",
        "        return \"N/A\"\n",
        "    return \"N/A\"\n",
        "\n",
        "for i in range(len(links)):\n",
        "    try:\n",
        "        link = links[i].get('href')\n",
        "        product_link_full = \"https://amazon.in\" + link\n",
        "        product_url = f'http://api.scraperapi.com?api_key={API_KEY}&url={product_link_full}'\n",
        "\n",
        "        # Request product page\n",
        "        product_webpage = requests.get(product_url)\n",
        "        soup = BeautifulSoup(product_webpage.content, \"html.parser\")\n",
        "\n",
        "        # Extract details including Best Seller Rank\n",
        "        details = {\n",
        "            'Product URL': product_link_full,\n",
        "            'Product Title': soup.find(\"span\", attrs={\"id\": 'productTitle'}).text.strip() if soup.find(\"span\", attrs={\"id\": 'productTitle'}) else \"N/A\",\n",
        "            'Brand': soup.find(\"span\", attrs={\"class\": 'a-size-base po-break-word'}).text.strip() if soup.find(\"span\", attrs={\"class\": 'a-size-base po-break-word'}) else \"N/A\",\n",
        "            'Price': soup.find(\"span\", attrs={\"class\": 'a-price aok-align-center reinventPricePriceToPayMargin priceToPay'}).text.strip() if soup.find(\"span\", attrs={\"class\": 'a-price aok-align-center reinventPricePriceToPayMargin priceToPay'}) else \"N/A\",\n",
        "            'Rating': soup.find(\"span\", attrs={\"class\": 'a-size-base a-color-base'}).text.strip() if soup.find(\"span\", attrs={\"class\": 'a-size-base a-color-base'}) else \"N/A\",\n",
        "            'Rating Count': soup.find(\"span\", attrs={\"id\": 'acrCustomerReviewText'}).text.strip() if soup.find(\"span\", attrs={\"id\": 'acrCustomerReviewText'}) else \"N/A\",\n",
        "            'Best Seller Rank': fetch_best_seller_rank(soup)  # Fetch and add Best Seller Rank\n",
        "        }\n",
        "\n",
        "        # Append extracted details to product data\n",
        "        product_data.append(details)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing product at index {i}: {e}\")\n",
        "\n",
        "# Save data to a CSV file\n",
        "df = pd.DataFrame(product_data)\n",
        "df.to_csv(\"/content/csvfiles/product_data7.csv\", index=False)\n",
        "print(\"Data saved to product_data.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Define the path to your CSV files\n",
        "path = '/Users/saiganeshreddykodekandla/Documents/projects/circuitTechnologies/circuitTechnologiesAmazon/aDataScrapingAmazon/csvFiles/*.csv'\n",
        "# Use glob to get all CSV files in the folder\n",
        "all_files = glob.glob(path)\n",
        "\n",
        "# Create an empty list to hold the DataFrames\n",
        "dataframes = []\n",
        "\n",
        "# Loop through each file and read it into a DataFrame\n",
        "for file in all_files:\n",
        "    df = pd.read_csv(file)\n",
        "    dataframes.append(df)\n",
        "\n",
        "# Concatenate all DataFrames into a single DataFrame\n",
        "merged_data = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "# Save the merged DataFrame to a new CSV file\n",
        "merged_data.to_csv('/Users/saiganeshreddykodekandla/Documents/projects/circuitTechnologies/circuitTechnologiesAmazon/csv_files/rawDataAmazonScrap.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
